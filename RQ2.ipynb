{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc8B6TqviEzZ"
      },
      "source": [
        "# Install Requirements\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bGOe4JjiNaM"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "S4B-U24zhXgq"
      },
      "source": [
        "# Import Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_jzdquwahXgw"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "from utils import (\n",
        "    apply_heuristics,\n",
        "    apply_heuristic_in_file,\n",
        "    adjust_spaces,\n",
        "    get_bleu_and_codebleu,\n",
        "    get_env_variable,\n",
        "    get_predictions_from_openai_and_write_to_file,\n",
        "    read_raw_tufano_dataset_from_csv,\n",
        "    write_list_to_file,\n",
        "    read_dataset,\n",
        "    get_EM_R4R,\n",
        "    get_EM,\n",
        "    format_file,\n",
        "    transfer_content_to_another_file,\n",
        "    get_predictions_from_edit_api_and_write_to_file,\n",
        "    get_few_shot_predictions_from_openai_and_write_to_file\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KUrL4Gr4hXg2"
      },
      "source": [
        "## Set OpenAI API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxwpo3xhhXg2"
      },
      "outputs": [],
      "source": [
        "openai.api_key = \"<YOUR_OPEN_API_KEY>\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hPIAsiJBhXg2"
      },
      "source": [
        "# Zero-Shot Inference and Evaluation using GPT-3.5-Turbo Model\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mUfG6JYyodZQ"
      },
      "source": [
        "## Dataset Path Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZHuHx6eNogvU"
      },
      "outputs": [],
      "source": [
        "OUTPUT_DIRECTORY = \"zero_shot_outputs\"\n",
        "TUFANO_SOURCE_FILE_PATH = \"datasets/tufano/test_CC_src.txt\"\n",
        "TUFANO_TARGET_FILE_PATH = \"datasets/tufano/test_CC_tgt.txt\"\n",
        "TUFANO_RAW_DATASET_FILE_PATH = \"datasets/tufano/raw_test.csv\"\n",
        "R4R_SOURCE_FILE_PATH = \"datasets/R4R/test_CC_src.txt\"\n",
        "R4R_TARGET_FILE_PATH = \"datasets/R4R/test_CC_tgt.txt\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "btBkamQ1hXg0"
      },
      "source": [
        "## Inference and Evaluation on Review4Repair Dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "83G5pZ4HlGqa"
      },
      "source": [
        "### data load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "P48lOolOhXg0",
        "outputId": "91eb5d23-33d6-4f20-8424-9ba70c7c5f57"
      },
      "outputs": [],
      "source": [
        "code_reviews, buggy_codes, target_codes = read_dataset(\n",
        "    dataset_name=\"R4R\", source_file_path=R4R_SOURCE_FILE_PATH, target_file_path=R4R_TARGET_FILE_PATH\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0sGNSfixhXg3"
      },
      "source": [
        "### inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7b0_QUUhXg3"
      },
      "outputs": [],
      "source": [
        "get_predictions_from_openai_and_write_to_file(\n",
        "    f\"{OUTPUT_DIRECTORY}/zero_shot_r4r_predictions_raw_no_heuristic.txt\",\n",
        "    f\"{OUTPUT_DIRECTORY}/zero_shot_r4r_ground_truths_raw_no_heuristic.txt\",\n",
        "    code_reviews,\n",
        "    buggy_codes,\n",
        "    target_codes\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lPSE_SNYusZj"
      },
      "source": [
        "### Before applying heuristics"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BhH_xk--usZr"
      },
      "source": [
        "#### accuracy (EM) calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9uyNPfRusZs"
      },
      "outputs": [],
      "source": [
        "get_EM(\n",
        "    f\"{OUTPUT_DIRECTORY}/zero_shot_r4r_predictions_raw.txt\",\n",
        "    f\"{OUTPUT_DIRECTORY}/zero_shot_r4r_ground_truths_raw.txt\",\n",
        "    \"R4R\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QYLe2VmvusZt"
      },
      "source": [
        "#### BLEU and CodeBLEU calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jOzHBEpusZt"
      },
      "outputs": [],
      "source": [
        "get_bleu_and_codebleu(\n",
        "    f\"{OUTPUT_DIRECTORY}/zero_shot_r4r_predictions_raw.txt\",\n",
        "    f\"{OUTPUT_DIRECTORY}/zero_shot_r4r_ground_truths_raw.txt\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F5XdI-U_usZu"
      },
      "source": [
        "### After applying heuristics"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Apply Heuristic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "apply_heuristic_in_file(f\"{OUTPUT_DIRECTORY}/zero_shot_r4r_predictions_raw.txt\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "okDiZj8LusZu"
      },
      "source": [
        "#### accuracy (EM) calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vW8g_7QusZv"
      },
      "outputs": [],
      "source": [
        "get_EM(\n",
        "    # f\"{OUTPUT_DIRECTORY}/zero_shot_r4r_predictions_raw_applied_heuristics_also_manual.txt\",\n",
        "    f\"{OUTPUT_DIRECTORY}/zero_shot_r4r_predictions_raw_applied_heuristics.txt\",\n",
        "    f\"{OUTPUT_DIRECTORY}/zero_shot_r4r_ground_truths_raw_target_only.txt\",\n",
        "    \"R4R\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "C6gqhgVgusZv"
      },
      "source": [
        "#### BLEU and CodeBLEU calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X80vbJk3usZw"
      },
      "outputs": [],
      "source": [
        "get_bleu_and_codebleu(\n",
        "    # f\"{OUTPUT_DIRECTORY}/zero_shot_r4r_predictions_raw_applied_heuristics_also_manual.txt\",\n",
        "    f\"{OUTPUT_DIRECTORY}/zero_shot_r4r_predictions_raw_applied_heuristics.txt\",\n",
        "    f\"{OUTPUT_DIRECTORY}/zero_shot_r4r_ground_truths_raw.txt\",\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "o3iKMpmBlzBd"
      },
      "source": [
        "## Inference and Evaluation on Tufano Dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "N06D324-l3yl"
      },
      "source": [
        "### data load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRODuW4hhXg1"
      },
      "outputs": [],
      "source": [
        "code_reviews, buggy_codes, target_codes = read_raw_tufano_dataset_from_csv(TUFANO_RAW_DATASET_FILE_PATH)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gcH6retjhXg4"
      },
      "source": [
        "### inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qMiJkSchXg4"
      },
      "outputs": [],
      "source": [
        "get_predictions_from_openai_and_write_to_file(\n",
        "    f\"{OUTPUT_DIRECTORY}/zero_shot_tufano_predictions_raw_no_heuristic.txt\",\n",
        "    f\"{OUTPUT_DIRECTORY}/zero_shot_tufano_ground_truths_raw.txt\",\n",
        "    code_reviews,\n",
        "    buggy_codes,\n",
        "    target_codes\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EXUpZEKIuzrD"
      },
      "source": [
        "### Before applying heuristics"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AFrNZiGLuzrQ"
      },
      "source": [
        "#### accuracy (EM) calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GWbivmczuzrR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EM: 17.86%\n"
          ]
        }
      ],
      "source": [
        "get_EM(\n",
        "    f\"{OUTPUT_DIRECTORY}/zero_shot_tufano_predictions_raw.txt\",\n",
        "    f\"{OUTPUT_DIRECTORY}/zero_shot_tufano_ground_truths_raw.txt\",\n",
        "    \"tufano\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ3T2gOWuzrS"
      },
      "source": [
        "#### BLEU and CodeBLEU calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DiokjfGuzrT"
      },
      "outputs": [],
      "source": [
        "get_bleu_and_codebleu(\n",
        "    f\"{OUTPUT_DIRECTORY}/zero_shot_tufano_predictions_raw.txt\",\n",
        "    f\"{OUTPUT_DIRECTORY}/zero_shot_tufano_ground_truths_raw.txt\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ICGsJzFduzrU"
      },
      "source": [
        "### After applying heuristics"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Apply Heuristic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "apply_heuristic_in_file(f\"{OUTPUT_DIRECTORY}/zero_shot_tufano_predictions_raw.txt\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Iat3-inhuzrV"
      },
      "source": [
        "#### accuracy (EM) calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "U2eDuDJZuzrV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EM: 30.66%\n"
          ]
        }
      ],
      "source": [
        "get_EM(\n",
        "    f\"{OUTPUT_DIRECTORY}/zero_shot_tufano_predictions_raw_applied_heuristics.txt\",\n",
        "    f\"{OUTPUT_DIRECTORY}/zero_shot_tufano_ground_truths_raw.txt\",\n",
        "    \"tufano\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "APk6EH9auzrW"
      },
      "source": [
        "#### BLEU and CodeBLEU calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Axmt8joLuzrW"
      },
      "outputs": [],
      "source": [
        "get_bleu_and_codebleu(\n",
        "    f\"{OUTPUT_DIRECTORY}/zero_shot_tufano_predictions_raw_applied_heuristics.txt\",\n",
        "    f\"{OUTPUT_DIRECTORY}/zero_shot_tufano_ground_truths_raw.txt\",\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tmwfcT5wn0HQ"
      },
      "source": [
        "# Few Shot Inference and Evaluation using GPT-3.5-Turbo Model\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zbKgYrNHoUN8"
      },
      "source": [
        "## Dataset path definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-B-BATcoYGQ"
      },
      "outputs": [],
      "source": [
        "OUTPUT_DIRECTORY = \"few_shot_outputs\"\n",
        "TUFANO_RAW_TRAIN_DATASET_FILE_PATH = \"datasets/tufano/raw_train.csv\"\n",
        "TUFANO_RAW_TEST_DATASET_FILE_PATH = \"datasets/tufano/raw_test.csv\"\n",
        "\n",
        "R4R_TRAIN_DATASET_SRC_FILE_PATH = \"datasets/R4R/train_CC_src.txt\"\n",
        "R4R_TRAIN_DATASET_TGT_FILE_PATH = \"datasets/R4R/train_CC_tgt.txt\"\n",
        "R4R_TEST_DATASET_SRC_FILE_PATH = \"datasets/R4R/test_CC_src.txt\"\n",
        "R4R_TEST_DATASET_TGT_FILE_PATH = \"datasets/R4R/test_CC_tgt.txt\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XD025EtFn0HT"
      },
      "source": [
        "## Inference and Evaluation on Review4Repair Dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SLHbf9jZn0HU"
      },
      "source": [
        "### data load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2G67qJWn0HV"
      },
      "outputs": [],
      "source": [
        "train_code_reviews, train_buggy_codes, train_target_codes = read_dataset(\n",
        "    dataset_name=\"R4R\",\n",
        "    source_file_path=R4R_TRAIN_DATASET_SRC_FILE_PATH,\n",
        "    target_file_path=R4R_TRAIN_DATASET_TGT_FILE_PATH\n",
        ")\n",
        "\n",
        "test_code_reviews, test_buggy_codes, test_target_codes = read_dataset(\n",
        "    dataset_name=\"R4R\",\n",
        "    source_file_path=R4R_TEST_DATASET_SRC_FILE_PATH,\n",
        "    target_file_path=R4R_TEST_DATASET_TGT_FILE_PATH\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hVDzo5ton0HW"
      },
      "source": [
        "### inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mzolr8Wn0HX"
      },
      "outputs": [],
      "source": [
        "get_few_shot_predictions_from_openai_and_write_to_file(\n",
        "    prediction_file_path=f\"{OUTPUT_DIRECTORY}/few_shot_r4r_predictions_raw_no_heuristic.txt\",\n",
        "    ground_truth_path=f\"{OUTPUT_DIRECTORY}/few_shot_r4r_ground_truths_raw_no_heuristic.txt\",\n",
        "    train_dataset=(train_code_reviews, train_buggy_codes, train_target_codes),\n",
        "    test_dataset=(test_code_reviews, test_buggy_codes, test_target_codes),\n",
        "    top_k=3\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MbXvMeVVthl6"
      },
      "source": [
        "### Before applying heuristics"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NXJE7o5Kthl8"
      },
      "source": [
        "#### accuracy (EM) calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jy3RguC6thl9"
      },
      "outputs": [],
      "source": [
        "get_EM(\n",
        "    f\"{OUTPUT_DIRECTORY}/few_shot_r4r_predictions_raw.txt\",\n",
        "    f\"{OUTPUT_DIRECTORY}/few_shot_r4r_ground_truths_raw.txt\",\n",
        "    \"R4R\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "m_wvjImjthl-"
      },
      "source": [
        "#### BLEU and CodeBLEU calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htgI6Chithl-"
      },
      "outputs": [],
      "source": [
        "get_bleu_and_codebleu(\n",
        "    f\"{OUTPUT_DIRECTORY}/few_shot_r4r_predictions_raw.txt\",\n",
        "    f\"{OUTPUT_DIRECTORY}/few_shot_r4r_ground_truths_raw.txt\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb2SvENhthl_"
      },
      "source": [
        "### After applying heuristics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Apply Heuristic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "apply_heuristic_in_file(f\"{OUTPUT_DIRECTORY}/few_shot_r4r_predictions_raw.txt\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3zqI4UjNthl_"
      },
      "source": [
        "#### accuracy (EM) calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2zl4Hu-thmA"
      },
      "outputs": [],
      "source": [
        "get_EM(\n",
        "    f\"{OUTPUT_DIRECTORY}/few_shot_r4r_predictions_raw_applied_heuristics.txt\",\n",
        "    f\"{OUTPUT_DIRECTORY}/few_shot_r4r_ground_truths_raw_target_only.txt\",\n",
        "    \"R4R\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "agLOFY_qthmA"
      },
      "source": [
        "#### BLEU and CodeBLEU calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ntg9GqPthmB"
      },
      "outputs": [],
      "source": [
        "get_bleu_and_codebleu(\n",
        "    f\"{OUTPUT_DIRECTORY}/few_shot_r4r_predictions_raw_applied_heuristics.txt\",\n",
        "    f\"{OUTPUT_DIRECTORY}/few_shot_r4r_ground_truths_raw.txt\",\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MFtd-ZuXn0HZ"
      },
      "source": [
        "## Inference and Evaluation on Tufano Dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EFxDXGdDn0HZ"
      },
      "source": [
        "### data load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMoZrc16n0Ha"
      },
      "outputs": [],
      "source": [
        "test_code_reviews, test_buggy_codes, test_target_codes = read_raw_tufano_dataset_from_csv(\n",
        "    TUFANO_RAW_TEST_DATASET_FILE_PATH\n",
        ")\n",
        "train_code_reviews, train_buggy_codes, train_target_codes = read_raw_tufano_dataset_from_csv(\n",
        "    TUFANO_RAW_TRAIN_DATASET_FILE_PATH\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mZj5qw7Fn0Ha"
      },
      "source": [
        "### inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUfXtuqGn0Ha"
      },
      "outputs": [],
      "source": [
        "get_few_shot_predictions_from_openai_and_write_to_file(\n",
        "    prediction_file_path=f\"{OUTPUT_DIRECTORY}/few_shot_tufano_predictions_raw_no_heuristic.txt\",\n",
        "    ground_truth_path=f\"{OUTPUT_DIRECTORY}/few_shot_tufano_ground_truths_raw_no_heuristic.txt\",\n",
        "    train_dataset=(train_code_reviews, train_buggy_codes, train_target_codes),\n",
        "    test_dataset=(test_code_reviews, test_buggy_codes, test_target_codes),\n",
        "    top_k=3\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yjssPVKItn9F"
      },
      "source": [
        "### Before applying heuristics"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XXM8Mfq5tn9G"
      },
      "source": [
        "#### accuracy (EM) calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNLWwWy0tn9H"
      },
      "outputs": [],
      "source": [
        "get_EM(\n",
        "    f\"{OUTPUT_DIRECTORY}/few_shot_tufano_predictions_raw.txt\",\n",
        "    f\"{OUTPUT_DIRECTORY}/few_shot_tufano_ground_truths_raw.txt\",\n",
        "    \"tufano\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "msw0uQ9vtn9H"
      },
      "source": [
        "#### BLEU and CodeBLEU calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTN4cHi0tn9I"
      },
      "outputs": [],
      "source": [
        "get_bleu_and_codebleu(\n",
        "    f\"{OUTPUT_DIRECTORY}/few_shot_tufano_predictions_raw.txt\",\n",
        "    f\"{OUTPUT_DIRECTORY}/few_shot_tufano_ground_truths_raw.txt\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5fgzDf2Ltn9I"
      },
      "source": [
        "### After applying heuristics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Apply Heuristic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "apply_heuristic_in_file(f\"{OUTPUT_DIRECTORY}/few_shot_tufano_predictions_raw.txt\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QEAAIAGptn9I"
      },
      "source": [
        "#### accuracy (EM) calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsVdlg2Btn9J"
      },
      "outputs": [],
      "source": [
        "get_EM(\n",
        "    f\"{OUTPUT_DIRECTORY}/few_shot_tufano_predictions_raw_applied_heuristics.txt\",\n",
        "    f\"{OUTPUT_DIRECTORY}/few_shot_tufano_ground_truths_raw.txt\",\n",
        "    \"tufano\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7cne1BFqtn9J"
      },
      "source": [
        "#### BLEU and CodeBLEU calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nv9IT8qYtn9K"
      },
      "outputs": [],
      "source": [
        "get_bleu_and_codebleu(\n",
        "    f\"{OUTPUT_DIRECTORY}/few_shot_tufano_predictions_raw_applied_heuristics.txt\",\n",
        "    f\"{OUTPUT_DIRECTORY}/few_shot_tufano_ground_truths_raw.txt\",\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6dPcplQ_qPD8"
      },
      "source": [
        "# Code-DaVinci-Edit-001 Inference and Evaluation\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MegNvKWZqPD_"
      },
      "source": [
        "## Dataset Path Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WEEa11yqPEA"
      },
      "outputs": [],
      "source": [
        "OUTPUT_DIRECTORY = \"edit_outputs\"\n",
        "TUFANO_SOURCE_FILE_PATH = \"datasets/tufano/test_CC_src.txt\"\n",
        "TUFANO_TARGET_FILE_PATH = \"datasets/tufano/test_CC_tgt.txt\"\n",
        "TUFANO_RAW_DATASET_FILE_PATH = \"datasets/tufano/raw_test.csv\"\n",
        "R4R_SOURCE_FILE_PATH = \"datasets/R4R/test_CC_src.txt\"\n",
        "R4R_TARGET_FILE_PATH = \"datasets/R4R/test_CC_tgt.txt\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wxN7iGFeqPEB"
      },
      "source": [
        "## Inference and Evaluation on Review4Repair Dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7DQ8yHp2qPEB"
      },
      "source": [
        "### data load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "p11hMWvVqPEB",
        "outputId": "91eb5d23-33d6-4f20-8424-9ba70c7c5f57"
      },
      "outputs": [],
      "source": [
        "code_reviews, buggy_codes, target_codes = read_dataset(\n",
        "    dataset_name=\"R4R\", source_file_path=R4R_SOURCE_FILE_PATH, target_file_path=R4R_TARGET_FILE_PATH\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Udk_O-h3qPEC"
      },
      "source": [
        "### inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg5N7Jn8qPED"
      },
      "outputs": [],
      "source": [
        "get_predictions_from_edit_api_and_write_to_file(\n",
        "    f\"{OUTPUT_DIRECTORY}/edit_r4r_predictions.txt\",\n",
        "    f\"{OUTPUT_DIRECTORY}/edit_r4r_ground_truths.txt\",\n",
        "    code_reviews,\n",
        "    buggy_codes,\n",
        "    target_codes\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zXZVk53JqPED"
      },
      "source": [
        "### accuracy (EM) calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5x4HYKxfqPED"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0YSU4tyiqPEE"
      },
      "source": [
        "### BLEU and CodeBLEU calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlDWyf4wqPEE"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aw0f3jdTqPEE"
      },
      "source": [
        "## Inference and Evaluation on Tufano Dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rephClEbqPEF"
      },
      "source": [
        "### data load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQwoFJVfqPEF"
      },
      "outputs": [],
      "source": [
        "code_reviews, buggy_codes, target_codes = read_raw_tufano_dataset_from_csv(TUFANO_RAW_DATASET_FILE_PATH)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8OJJ4gb-qPEF"
      },
      "source": [
        "### inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfVDlUo2qPEG"
      },
      "outputs": [],
      "source": [
        "get_predictions_from_edit_api_and_write_to_file(\n",
        "    f\"{OUTPUT_DIRECTORY}/edit_tufano_predictions.txt\",\n",
        "    f\"{OUTPUT_DIRECTORY}/edit_tufano_ground_truths.txt\",\n",
        "    code_reviews,\n",
        "    buggy_codes,\n",
        "    target_codes\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "P9Cxj0l6qPEG"
      },
      "source": [
        "### accuracy (EM) calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vtAQYjhYqPEG"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_B49hv2bqPEH"
      },
      "source": [
        "### BLEU and CodeBLEU calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DTHlKFzUqPEH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
